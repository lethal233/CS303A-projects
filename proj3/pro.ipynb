{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data set - training data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.json\",'r') as load_f:\n",
    "    load_data = json.load(load_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "t = range(0, 100)   # 范围在0到100之间，需要用到range()函数。\n",
    "test_index = random.sample(range(0, len(load_data)), int(len(load_data)*0.2))\n",
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "for i in range(len(load_data)):\n",
    "    if i in test_index:\n",
    "        test_data.append(load_data[i]['data'])\n",
    "        test_label.append(load_data[i]['label'])\n",
    "    else:\n",
    "        train_data.append(load_data[i]['data'])\n",
    "        train_label.append(load_data[i]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 10227)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting features from text files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "max_df = 0.999 # 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "min_df = 0.001 # 在低于这一数量的文档中出现的关键词（过于独特），去除掉。\n",
    "count_vect = CountVectorizer(max_df = max_df,\n",
    "                       min_df = min_df,\n",
    "                       lowercase = False)\n",
    "X_train_counts = count_vect.fit_transform(train_data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "# The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "# We will be using the 'text_clf' going forward.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8658"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance of NB Classifier\n",
    "import numpy as np\n",
    "predicted = text_clf.predict(test_data)\n",
    "np.mean(predicted == test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8534"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(train_data, train_label)\n",
    "predicted_svm = text_clf_svm.predict(test_data)\n",
    "np.mean(predicted_svm == test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train number: 17500\n",
      "test number: 7500\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "import numpy\n",
    "import json\n",
    "\n",
    "#调整了格式，一行是一条数据\n",
    "def inputdata(filename):\n",
    "    with open(filename,'r') as load_f:\n",
    "        load_data = json.load(load_f)\n",
    "    return load_data\n",
    "\n",
    "def splitset(trainset,testset):\n",
    "    train_words = []\n",
    "    train_tags = []\n",
    "    test_words = []\n",
    "    test_tags = []\n",
    "    for i in trainset:\n",
    "        # index = i.index(':')\n",
    "        train_words.append(i['data'])\n",
    "        # print i\n",
    "        train_tags.append(int(i['label']))\n",
    "\n",
    "    for i in testset:\n",
    "        # index = i.index(':')\n",
    "        test_words.append(i['data'])\n",
    "        # print i\n",
    "        test_tags.append(int(i['label']))\n",
    "\n",
    "    return train_words,train_tags,test_words,test_tags\n",
    "\n",
    "#完成打开文件后的准备工作\n",
    "\n",
    "\n",
    "def tfvectorize(train_words,test_words):\n",
    "    v = TfidfVectorizer(tokenizer=comma_tokenizer,binary = False, decode_error = 'ignore',stop_words = 'english')\n",
    "    train_data = v.fit_transform(train_words)\n",
    "    test_data = v.transform(test_words)\n",
    "    return train_data,test_data\n",
    "\n",
    "#按比例划分训练集与测试集\n",
    "def splitDataset(dataset,splitRatio):\n",
    "    trainSize = int(len(dataset)*splitRatio)\n",
    "    trainSet = []\n",
    "    copy = dataset\n",
    "    while len(trainSet)<trainSize:\n",
    "        index = random.randrange(len(copy))\n",
    "        trainSet.append(copy.pop(index))\n",
    "    return trainSet,copy\n",
    "\n",
    "#得到准确率和召回率\n",
    "def evaluate(actual, pred):\n",
    "    m_precision = metrics.precision_score(actual, pred,average='macro')\n",
    "    m_recall = metrics.recall_score(actual,pred,average='macro')\n",
    "    print( 'precision:{0:.3f}'.format(m_precision))\n",
    "    print ('recall:{0:0.3f}'.format(m_recall))\n",
    "\n",
    "#创建svm分类器\n",
    "def train_clf(train_data, train_tags):\n",
    "    clf = svm.SVC(C=0.1, cache_size=200, class_weight=None, decision_function_shape='ovr'\n",
    "                  , kernel='linear', max_iter=-1, probability=True, random_state=42, shrinking=True,\n",
    "                  tol=0.0001, verbose=False)\n",
    "    clf.fit(train_data, numpy.asarray(train_tags))\n",
    "\n",
    "    return clf\n",
    "\n",
    "def covectorize(train_words,test_words):\n",
    "    max_df = 0.999 # 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "    min_df = 0.001 # 在低于这一数量的文档中出现的关键词（过于独特），去除掉。\n",
    "    count_vect = CountVectorizer(max_df = max_df,\n",
    "                           min_df = min_df,\n",
    "                           lowercase = False)    \n",
    "    train_data = count_vect.fit_transform(train_words)\n",
    "    test_data = count_vect.transform(test_words)\n",
    "    return train_data,test_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    linelist = inputdata('./train.json')\n",
    "    # for i in linelist:\n",
    "    #     print i.decode('utf-8')\n",
    "\n",
    "    # 划分成两个list\n",
    "    trainset, testset = splitDataset(linelist, 0.7)\n",
    "    # for i in trainset:\n",
    "    #     print i.decode('utf-8')\n",
    "    print ('train number:', len(trainset))\n",
    "    print ('test number:', len(testset))\n",
    "    \n",
    "\n",
    "    train_words, train_tags, test_words, test_tags = splitset(trainset, testset)\n",
    "    \n",
    "    # for i in train_words:\n",
    "    #     print i\n",
    "    # for i in train_tags:\n",
    "    #     print i\n",
    "    # for i in numpy.asarray(train_tags):\n",
    "    #     print i\n",
    "    # for i in test_words:\n",
    "    #     print i\n",
    "    # for i in test_tags:\n",
    "    #     print i\n",
    "\n",
    "\n",
    "    # train_data, test_data = tfvectorize(train_words, test_words)\n",
    "    train_data, test_data = covectorize(train_words, test_words)\n",
    "    # for i in test_data:\n",
    "    #     print i\n",
    "    clf = train_clf(train_data,train_tags)\n",
    "\n",
    "    re =  clf.predict(test_data)\n",
    "    # print re\n",
    "    evaluate(numpy.asarray(test_tags),re)\n",
    "    # print re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
